{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary package \n",
    "import yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring warning messages\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  4 of 4 completed\n"
     ]
    }
   ],
   "source": [
    "# Using the .download() method to get our data\n",
    "\n",
    "raw_data = yfinance.download (tickers = \"^GSPC ^FTSE ^N225 ^GDAXI\", start = \"1994-01-07\", end = \"2020-04-05\", interval = \"1d\", group_by = 'ticker', auto_adjust = True, treads = True)\n",
    "\n",
    "# tickers -> The time series we are interested in - (in our case, these are the S&P, FTSE, NIKKEI and DAX)\n",
    "# start -> The starting date of our data set\n",
    "# end -> The ending date of our data set (at the time of upload, this is the current date)\n",
    "# interval -> The distance in time between two recorded observations. Since we're using daily closing prices, we set it equal to \"1d\", which indicates 1 day. \n",
    "# group_by -> The way we want to group the scraped data. Usually we want it to be \"ticker\", so that we have all the information about a time series in 1 variable.\n",
    "# auto_adjust -> Automatically adjust the closing prices for each period. \n",
    "# treads - > Whether to use threads for mass downloading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a back up copy in case we remove/alter elements of the data by mistake\n",
    "df_comp = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns to the data set\n",
    "df_comp['spx'] = df_comp['^GSPC'].Close\n",
    "df_comp['dax'] = df_comp['^GDAXI'].Close\n",
    "df_comp['ftse'] = df_comp['^FTSE'].Close\n",
    "df_comp['nikkei'] = df_comp['^N225'].Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_comp = df_comp.iloc[1:] # Removing the first elements, since we always start 1 period before the first, due to time zone differences of closing prices\n",
    "del df_comp['^N225']  # Removing the original tickers of the data set\n",
    "del df_comp['^GSPC']\n",
    "del df_comp['^GDAXI']\n",
    "del df_comp['^FTSE']\n",
    "df_comp=df_comp.asfreq('b') # Setting the frequency of the data\n",
    "df_comp=df_comp.fillna(method='ffill') # Filling any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   spx          dax         ftse        nikkei\n",
      "                                                              \n",
      "Date                                                          \n",
      "1994-01-10  475.269989  2225.000000  3440.600098  18443.439453\n",
      "1994-01-11  474.130005  2228.100098  3413.800049  18485.250000\n",
      "1994-01-12  474.170013  2182.060059  3372.000000  18793.880859\n",
      "1994-01-13  472.470001  2142.370117  3360.000000  18577.259766\n",
      "1994-01-14  474.910004  2151.050049  3400.600098  18973.699219\n",
      "                    spx          dax         ftse        nikkei\n",
      "                                                               \n",
      "Date                                                           \n",
      "2020-03-30  2626.649902  9815.969727  5563.700195  19084.970703\n",
      "2020-03-31  2584.590088  9935.839844  5672.000000  18917.009766\n",
      "2020-04-01  2470.500000  9544.750000  5454.600098  18065.410156\n",
      "2020-04-02  2526.899902  9570.820312  5480.200195  17818.720703\n",
      "2020-04-03  2488.649902  9525.769531  5415.500000  17820.189453\n"
     ]
    }
   ],
   "source": [
    "print (df_comp.head()) # Displaying the first 5 elements to make sure the data was scraped correctly\n",
    "print (df_comp.tail()) # Making sure the last day we're including in the series are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp.to_csv(r'/your filepath/index2020.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
